---
permalink: conclusion-to-reframing-impact
lw-was-draft-post: "false"
lw-is-af: "true"
lw-is-debate: "false"
lw-page-url: https://www.lesswrong.com/posts/sHpiiZS2gPgoPnijX/conclusion-to-reframing-impact
lw-is-question: "false"
lw-posted-at: 2020-02-28T16:05:40.656000Z
lw-last-modification: 2023-02-24T01:22:38.985000Z
lw-curation-date: None
lw-frontpage-date: 2020-02-28T19:14:32.555000Z
lw-was-unlisted: "false"
lw-is-shortform: "false"
lw-num-comments-on-upload: 18
lw-base-score: 40
lw-vote-count: 14
af-base-score: 17
af-num-comments-on-upload: 18
publish: true
title: Conclusion to 'Reframing Impact'
lw-latest-edit: 2023-02-17T09:29:58.232000Z
lw-is-linkpost: "false"
tags:
  - impact-regularization
  - AI
aliases:
  - conclusion-to-reframing-impact
lw-sequence-title: Reframing Impact
lw-sequence-image-grid: sequencesgrid/izfzehxanx48hvf10lnl
lw-sequence-image-banner: sequences/zpia9omq0zfhpeyshvev
sequence-link: posts#reframing-impact
prev-post-slug: excitement-about-impact-measures
prev-post-title: Reasons for Excitement about Impact of Impact Measure Research
lw-reward-post-warning: "false"
use-full-width-images: "false"
date_published: 2020-02-28 00:00:00
original_url: https://www.lesswrong.com/posts/sHpiiZS2gPgoPnijX/conclusion-to-reframing-impact
skip_import: true
card_image: https://assets.turntrout.com/static/images/card_images/C0o5g91.png
description: The "Reframing Impact" sequence concludes with probability estimates
  for key claims.
date_updated: 2024-11-22 20:04:30.137574
---




![](https://assets.turntrout.com/static/images/posts/pbmk8ndyip6nyu4ntf6z.avif)![](https://assets.turntrout.com/static/images/posts/icddpmwoxx5ftcysxo8k.avif)![](https://assets.turntrout.com/static/images/posts/mxhzcdashtl5euloeolx.avif)![](https://assets.turntrout.com/static/images/posts/d1mqg6p4ghuweu4sth5u.avif)![](https://assets.turntrout.com/static/images/posts/veypvrfwfr1xwwz4zx8m.avif)![](https://assets.turntrout.com/static/images/posts/qanem2tu332ayspkhutk.avif)![](https://assets.turntrout.com/static/images/posts/lza8s3ncwyioba7gn5kc.avif)![](https://assets.turntrout.com/static/images/posts/h14cfepf9ggi4hnx6ub1.avif)![](https://assets.turntrout.com/static/images/posts/w4iaoloixtlxhc26zy67.avif)![](https://assets.turntrout.com/static/images/posts/sr4u489gcv8jfltydthi.avif)

> [!thanks] Acknowledgments
> After ~700 hours of work over the course of ~9 months, the sequence is finally complete.
>
> This work was made possible by the Center for Human-Compatible AI, the Berkeley Existential Risk Initiative, and the Long-Term Future Fund. Deep thanks to Rohin Shah, Abram Demski, Logan Smith, Evan Hubinger, `TheMajor`, Chase Denecke, Victoria Krakovna, Alper Dumanli, Cody Wild, Matthew Barnett, Daniel Blank, Sara Haxhia, Connor Flexman, Zack M. Davis, Jasmine Wang, Matthew Olson, Rob Bensinger, William Ellsworth, Davide Zagami, Ben Pace, and a million other people for giving feedback on this sequence.

# Appendix: Probability estimates

I've made many claims in these posts. All views are my own.

| Statement | Credence |
|:---------:|:---------|
| There exists a simple closed-form solution to catastrophe avoidance (in the outer alignment sense). | 25% |
| For the superhuman case, penalizing the agent for increasing its own Attainable Utility (AU) is better than penalizing the agent for increasing other AUs. | 65% |
| Some version of Attainable Utility Preservation solves side effect problems for an extremely wide class of real-world tasks and for subhuman agents. | 65% |
| The catastrophic convergence conjecture is true. That is, unaligned goals tend to have catastrophe-inducing optimal policies because of power-seeking incentives. | 70%[^ccc] |
| Agents trained by powerful RL algorithms on arbitrary reward signals generally try to take over the world. | 75%[^power] |
| AUP<sub>conceptual</sub> prevents catastrophe, assuming the catastrophic convergence conjecture. | 85% |
| Attainable Utility theory describes how people feel impacted. | 95% |

[^power]: [The theorems on power-seeking](https://arxiv.org/abs/1912.01683) only apply to optimal policies in fully observable environments, which isn't realistic for real-world agents. However, I think they're still informative. There are also strong intuitive arguments for power-seeking.

[^ccc]: There seems to be a dichotomy between "catastrophe directly incentivized by goal" and "catastrophe indirectly incentivized by goal through power-seeking", although `Vika` [provides intuitions in the other direction](https://www.lesswrong.com/posts/sHpiiZS2gPgoPnijX/conclusion-to-reframing-impact?commentId=6sxBzsh8yfwnPk4iH#6sxBzsh8yfwnPk4iH).

> [!note]
> [The LessWrong version of this post](https://www.lesswrong.com/posts/sHpiiZS2gPgoPnijX/conclusion-to-reframing-impact) contained probability estimates from other users.

# Appendix: Easter Eggs

The big art pieces (and especially the last illustration in this post) were designed to convey a specific meaning, the interpretation of which I leave to the reader.

There are a few pop culture references which I think are obvious enough to not need pointing out, and a lot of hidden smaller playfulness which doesn't quite rise to the level of "easter egg".

Reframing Impact
: The bird's nest contains a literal easter egg.
:
: ![](https://assets.turntrout.com/static/images/posts/hdlkd44jvawsxgpthbgi.avif)
:
: The paperclip-Balrog drawing contains a [Tengwar](https://en.wikipedia.org/wiki/Tengwar) inscription which reads "one measure to bind them", with "measure" in impact-blue and "them" in utility-pink.
:
: ![](https://assets.turntrout.com/static/images/posts/v7pzpzvi342b3svksbag.avif)
:
: "Towards a New Impact Measure" was the title of [the post](/towards-a-new-impact-measure) in which AUP was introduced.
:
: ![](https://assets.turntrout.com/static/images/posts/ynwdidys1i7yopyqerfh.avif)

<br/>

Attainable Utility Theory: Why Things Matter
:
: This style of maze is from the video game _Undertale_.
:
: ![](https://assets.turntrout.com/static/images/posts/olz9peoa2krvvorlgdn8.avif)

<br/>

Seeking Power is Instrumentally Convergent in MDPs
:
: To seek power, Frank is trying to get at the Infinity Gauntlet.
:
: ![](https://assets.turntrout.com/static/images/posts/pdqrmsxtawdzt2c7idez.avif)

<br/>

The tale of Frank and the orange Pebblehoarder
: Speaking of under-tales, a friendship has been blossoming right under our noses:
:
: ![](https://assets.turntrout.com/static/images/posts/dfog9czq2wdboz8m0dpv.avif)
Figure: After the Pebblehoarders suffer the devastating transformation of all of their pebbles into obsidian blocks, Frank generously gives away his favorite pink marble as a makeshift pebble.
:
:
: ![](https://assets.turntrout.com/static/images/posts/id8zdpzvvjsyyi9a9hfe.avif)
Figure: The title cuts to the middle of their adventures together, the Pebblehoarder showing its gratitude by helping Frank reach things high up.
:
:
: ![](https://assets.turntrout.com/static/images/posts/mx5gc86qpthgbzeypfw9.avif)
Figure: This still at the midpoint of the sequence is from [the final scene of _The Hobbit: An Unexpected Journey_](https://www.youtube.com/watch?v=KEegn1R601M), where the party is overlooking Erebor, the Lonely Mountain. They've made it through the Misty Mountains, only to find Smaug's abode looming in the distance.
:
: ![](https://assets.turntrout.com/static/images/posts/jdcmcy4bzxggxdallwok.avif)
Figure: And, at last, we find Frank and orange Pebblehoarder popping some of the champagne from Smaug's hoard. Since [Erebor isn't close to Gondor](https://assets.turntrout.com/static/images/posts/Map-of-Middle-Earth-lord-of-the-rings-2329809-1600-1200.avif), we don't see Frank and the Pebblehoarder gazing at [Ephel DÃºath](https://en.wikipedia.org/wiki/Mordor#Geography) from Minas Tirith.
