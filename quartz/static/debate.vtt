WEBVTT

1
00:00:00.640 --> 00:00:01.640
[Biden] What's up, Barack?

2
00:00:02.300 --> 00:00:03.340
[Trump] Hi there, Sleepy Joe.

3
00:00:04.200 --> 00:00:05.960
[Obama] For our viewers, let's explain what we're doing.

4
00:00:07.480 --> 00:00:14.460
[Obama] Eliezer Yudkowsky recently wrote a Time article about how AI will kill everyone unless we shut down the expensive training runs.

5
00:00:15.200 --> 00:00:20.300
[Obama] I read this article, and though I found it hyperbolic, I thought it made concerning points.

6
00:00:21.020 --> 00:00:26.500
[Obama] After consulting more with experts in and around the field, I concluded that Eliezer's stance is reasonable.

7
00:00:26.500 --> 00:00:27.280
[Obama] I agree.

8
00:00:28.120 --> 00:00:29.760
[Obama] We should shut down AI development.

9
00:00:30.920 --> 00:00:31.299
[Trump] Bigly.

10
00:00:31.700 --> 00:00:38.160
[Trump] I mean, I thought this sounded kind of bogus, to be honest, and I never thought I'd be agreeing with Barack Hussein Obama, but this issue is important.

11
00:00:38.940 --> 00:00:39.480
[Biden] I also...

12
00:00:39.480 --> 00:00:40.080
[Trump] Shut up, Joe.

13
00:00:40.160 --> 00:00:40.880
[Trump] No one cares.

14
00:00:41.260 --> 00:00:42.120
[Biden] Will you shut up, man?

15
00:00:42.380 --> 00:00:43.780
[Biden] This is so unpresidential.

16
00:00:44.540 --> 00:00:46.240
[Obama] Let's put aside these petty disputes.

17
00:00:46.940 --> 00:00:48.020
[Obama] Remember what we're here to do.

18
00:00:48.440 --> 00:00:48.820
[Biden] Fine.

19
00:00:49.540 --> 00:00:52.360
[Biden] As I was saying, I also found this article concerning.

20
00:00:53.100 --> 00:00:55.340
[Obama] So, we asked to see what we can do.

21
00:00:56.620 --> 00:01:03.560
[Obama] 80,000 Hours said that direct work probably isn't our comparative advantage, but apparently there aren't enough grant evaluators.

22
00:01:04.340 --> 00:01:12.820
[Obama] Since we aren't technical researchers ourselves, 80,000 Hours concluded that our time is not valuable, and they made us evaluate research areas.

23
00:01:13.500 --> 00:01:14.780
[Trump] Can you get to the point, Obama?

24
00:01:15.440 --> 00:01:18.440
[Obama] We're ranking alignment agendas today in a tier list.

25
00:01:18.900 --> 00:01:21.580
[Obama] We've all done a lot of reading, so let's share our thoughts.

26
00:01:22.940 --> 00:01:26.280
[Obama] Let's start off with something I think we can agree on, mechanistic interpretability.

27
00:01:26.860 --> 00:01:27.580
[Obama] Easy S tier?

28
00:01:28.140 --> 00:01:30.240
[Trump] Are you another interpretability simp, Barack?

29
00:01:30.860 --> 00:01:35.400
[Trump] Ever since King Eliezer started drooling over it, people just trip over themselves.

30
00:01:35.560 --> 00:01:36.440
[Trump] You wouldn't believe it.

31
00:01:36.880 --> 00:01:41.220
[Trump] They absolutely trip over themselves to show how much they love interpretability.

32
00:01:41.460 --> 00:01:43.220
[Obama] I notice you haven't answered my question.

33
00:01:43.700 --> 00:01:45.040
[Trump] Can we be specific, at least?

34
00:01:45.100 --> 00:01:47.060
[Trump] There's lots of kinds of interpretability work.

35
00:01:47.800 --> 00:01:51.200
[Obama] Let's start off with Anthropic's agenda, tackling superposition.

36
00:01:52.260 --> 00:01:53.960
[Biden] I think they do good, decent work.

37
00:01:54.440 --> 00:01:58.400
[Biden] God, I wish they'd pump the brakes on their capabilities progress, though.

38
00:01:58.740 --> 00:01:59.560
[Trump] Great idea, Joe.

39
00:01:59.680 --> 00:02:00.100
[Trump] Amazing.

40
00:02:00.700 --> 00:02:06.520
[Trump] That way we can be caught with our pants down when AGI comes, and we don't know shit about how to apply alignment techniques at scale.

41
00:02:07.000 --> 00:02:10.320
[Biden] Aren't you in court right now because you got caught with your pants down?

42
00:02:10.900 --> 00:02:12.800
[Biden] Don't you have other things to be doing, Donald?

43
00:02:13.380 --> 00:02:14.040
[Obama] Seriously, Joe?

44
00:02:15.820 --> 00:02:23.160
[Obama] Anyways, I think their superposition work has a solid chance of tackling a core interpretability challenge.

45
00:02:23.920 --> 00:02:32.760
[Obama] We can visualize attention patterns all day long, but to truly understand realistic transformer circuits, we've got to tackle the MLP layers.

46
00:02:33.880 --> 00:02:39.360
[Obama] After reading the toy models of superposition posts, I'm convinced that superposition is a substantial roadblock.

47
00:02:40.020 --> 00:02:42.100
[Biden] Yeah, sure, but I'm not too sold.

48
00:02:42.100 --> 00:02:44.820
[Biden] They do a good job, but you can never be sure with these things.

49
00:02:45.520 --> 00:02:48.840
[Biden] Appealingsounding alignment theorizing often seems useless in retrospect.

50
00:02:49.100 --> 00:02:50.020
[Biden] I vote for A tier.

51
00:02:50.240 --> 00:02:52.660
[Trump] You're one to talk about useless theory, Joe.

52
00:02:52.760 --> 00:02:58.580
[Trump] You're the one who keeps Discord messaging me these insane theories from the most abstract alignment agendas.

53
00:02:59.040 --> 00:03:02.460
[Trump] Like I really had no idea you could become that detached from reality.

54
00:03:02.840 --> 00:03:04.020
[Trump] I really didn't.

55
00:03:04.080 --> 00:03:07.260
[Trump] At least Anthropic runs experiments and interprets real models.

56
00:03:08.040 --> 00:03:12.660
[Trump] I mean, to be honest, guys, I think this superposition stuff isn't going to save us.

57
00:03:13.180 --> 00:03:15.040
[Trump] We solve superposition—fine.

58
00:03:15.160 --> 00:03:19.680
[Trump] We can now understand whatever tiny network their interp team is messing around with next year.

59
00:03:20.340 --> 00:03:22.680
[Obama] Donald, I think many researchers would disagree with you.

60
00:03:23.220 --> 00:03:31.920
[Obama] I've spoken to a few interpretability researchers who consider superposition to be a more fundamental challenge than scaling interpretability to realistic models.

61
00:03:32.420 --> 00:03:32.880
[Trump] You know what?

62
00:03:33.400 --> 00:03:35.960
[Trump] Okay, I'll go along with a high tier ranking for this one.

63
00:03:36.300 --> 00:03:40.520
[Trump] Superposition is interesting and concrete and at least has a shot of being important, you know?

64
00:03:41.200 --> 00:03:42.360
[Obama] I'm putting this in A tier.

65
00:03:44.440 --> 00:03:46.260
[Obama] Donald, you made a good point earlier.

66
00:03:46.860 --> 00:03:48.740
[Obama] There are several kinds of interpretability.

67
00:03:49.220 --> 00:03:51.680
[Obama] Let's talk about circuits interpretability now.

68
00:03:51.980 --> 00:03:53.000
[Biden] Why is that different?

69
00:03:53.740 --> 00:04:00.660
[Obama] It's true that Anthropic superposition agenda aims to enable circuit research by demystifying MLP layers.

70
00:04:01.260 --> 00:04:08.480
[Obama] However, solving superposition is its own problem and may yield benefits beyond just improving our ability to interpret circuits.

71
00:04:08.480 --> 00:04:10.780
[Trump] Circuit interpretability, for real?

72
00:04:11.460 --> 00:04:15.480
[Trump] Listen, guys, I know we like huffing hopium over here, but I just can't believe it.

73
00:04:15.740 --> 00:04:17.720
[Trump] I don't know what era these guys are living in.

74
00:04:17.760 --> 00:04:21.280
[Trump] Do they think GPT-4 is made out of, like, 10 circuits?

75
00:04:21.740 --> 00:04:25.420
[Trump] How are we possibly going to understand what any realistic model is doing?

76
00:04:25.860 --> 00:04:33.100
[Obama] Donald, I think the hope is to learn general facts about how models learn to make decisions to get faster at finding circuits.

77
00:04:33.840 --> 00:04:35.320
[Obama] Maybe we can even automate it.

78
00:04:35.340 --> 00:04:39.040
[Obama] I think that's a reasonable possibility and very much on the table.

79
00:04:39.340 --> 00:04:40.200
[Trump] Oh, yeah, bigly.

80
00:04:40.440 --> 00:04:41.700
[Trump] That's bigly going to happen.

81
00:04:41.800 --> 00:04:42.740
[Trump] We'll just automate it.

82
00:04:43.080 --> 00:04:46.800
[Trump] What do we have, like, four years left until this A.I. absolutely fucks us?

83
00:04:47.300 --> 00:04:49.140
[Trump] How are we going to automate this in time?

84
00:04:49.700 --> 00:04:56.340
[Obama] We can talk about causal scrubbing and other ideas for automating alignment research, but the real answer is that I don't know.

85
00:04:56.940 --> 00:04:58.380
[Obama] The situation looks pretty grim.

86
00:04:58.760 --> 00:05:00.340
[Obama] How are we going to do anything in time?

87
00:05:00.840 --> 00:05:03.540
[Biden] We should judge agendas by their relative merits, obviously.

88
00:05:04.240 --> 00:05:06.320
[Biden] Seems like we're leaning towards B tier.

89
00:05:06.940 --> 00:05:07.680
[Biden] That's hogwash.

90
00:05:07.860 --> 00:05:10.480
[Biden] Bigly Don pointed out that we aren't fast at it right now.

91
00:05:10.740 --> 00:05:16.420
[Biden] Fine. But circuits interpretability is our source of ground truth on how A.I. cognition develops.

92
00:05:16.740 --> 00:05:21.020
[Biden] That's like saying, uh, I'm not good at breathing right now, so breathing isn't important.

93
00:05:21.680 --> 00:05:26.460
[Biden] I think it's true that we're slow at interpretability and also true that it's extremely important to be good at it.

94
00:05:26.800 --> 00:05:31.080
[Trump] I must admit, I got to admit it, I wasn't thinking too much about relative merits, you know?

95
00:05:31.200 --> 00:05:32.680
[Trump] Sometimes I do get a bit impatient.

96
00:05:32.680 --> 00:05:33.380
[Trump] I really do.

97
00:05:33.900 --> 00:05:35.620
[Trump] But I swear to God, we got to get faster.

98
00:05:35.740 --> 00:05:37.220
[Trump] We need to get faster and better at this.

99
00:05:37.300 --> 00:05:37.920
[Trump] It's no joke.

100
00:05:38.520 --> 00:05:43.660
[Trump] Did you hear that Neel Nanda guy found some linear representations in Othello GPT in a single weekend?

101
00:05:44.180 --> 00:05:47.160
[Trump] Why does everything else in real life take like a billion years?

102
00:05:47.240 --> 00:05:48.100
[Trump] We got to get faster.

103
00:05:48.720 --> 00:05:50.160
[Biden] Why not show them how it's done, Donald?

104
00:05:50.520 --> 00:05:53.980
[Trump] I'm planning on it right after this video, if we ever finish this up.

105
00:05:54.240 --> 00:05:59.040
[Trump] I'm going to figure out that algebraic value editing thing that Team Shard talked about a few weeks ago.

106
00:05:59.540 --> 00:06:06.940
[Trump] God, finally, Team Shard has an exciting idea, and then they take fucking forever to release even the smallest piece of information about it.

107
00:06:07.360 --> 00:06:08.700
[Obama] Guys, back on topic, please.

108
00:06:09.040 --> 00:06:11.360
[Trump] Yeah, whatever, either B or A tier is fine with me.

109
00:06:11.460 --> 00:06:12.560
[Biden] It belongs in A tier.

110
00:06:12.700 --> 00:06:13.820
[Obama] I'm putting it in A tier.

111
00:06:14.820 --> 00:06:19.260
[Obama] I think this is a promising direction, but the rubber isn't hitting the road in quite the right way.

112
00:06:20.200 --> 00:06:27.840
[Obama] I applaud the existing talent and effort being poured into the field, and I also think that there's a lot of room for improvement and growth.

113
00:06:29.780 --> 00:06:34.140
[Obama] Let's talk about the Machine Intelligence Research Institute's Agent Foundations agenda.

114
00:06:35.120 --> 00:06:41.160
[Obama] This agenda wants to better understand realistic reasoning systems which don't have infinite time to think about how the world works.

115
00:06:41.160 --> 00:06:43.420
[Trump] Instant F tier content, I'll tell you right now.

116
00:06:43.580 --> 00:06:45.860
[Biden] I've about had it with you and your shit-tier takes, Donald.

117
00:06:46.860 --> 00:06:51.480
[Biden] Agent Foundations tackles key confusions about what agents are and what alignment even means.

118
00:06:51.500 --> 00:06:52.820
[Trump] That's what you like to tell yourself, huh?

119
00:06:52.880 --> 00:07:02.340
[Biden] While it's good to understand what modern systems are doing, man, I sure get it, how it can feel good to make a smart language model output helpful things.

120
00:07:02.940 --> 00:07:05.640
[Biden] I think that's not tackling the brunt of the problem.

121
00:07:06.320 --> 00:07:10.060
[Biden] To do that, we have to really know what the hell we're doing.

122
00:07:10.340 --> 00:07:13.720
[Trump] Yeah, maybe we do need to really know what we're doing, but let's be clear.

123
00:07:13.920 --> 00:07:15.400
[Trump] Let's be absolutely clear here.

124
00:07:15.740 --> 00:07:23.060
[Trump] I don't think Agent Foundations, the actual agenda we're discussing, Sleepy Joe, this agenda doesn't know what it's talking about.

125
00:07:23.560 --> 00:07:24.200
[Trump] You know, I agree.

126
00:07:24.200 --> 00:07:27.140
[Trump] I'm one of the people who most thinks we should know what we're doing here.

127
00:07:27.540 --> 00:07:29.460
[Trump] We should slow down and take our time.

128
00:07:29.920 --> 00:07:33.760
[Trump] We should do that in ideal alternate society, utopia, fantasy land.

129
00:07:34.100 --> 00:07:40.740
[Trump] But back in reality, this agenda has made zero progress, zero that helps Trump Tower not get consumed by AI.

130
00:07:41.240 --> 00:07:46.540
[Obama] Donald, you can be a little hyperbolic, and it detracts from the points you're trying to make.

131
00:07:47.700 --> 00:07:50.000
[Obama] Functional decision theory was a real insight.

132
00:07:51.700 --> 00:08:00.460
[Obama] I've definitely appreciated having those concepts in my mental tool belt, and logical induction was, you know, pretty cool.

133
00:08:00.880 --> 00:08:01.220
[Trump] Cool?

134
00:08:01.520 --> 00:08:02.360
[Trump] Sure, it's cool.

135
00:08:02.420 --> 00:08:03.080
[Trump] But does it matter?

136
00:08:03.200 --> 00:08:04.600
[Trump] We've got to stop fucking around.

137
00:08:04.700 --> 00:08:05.280
[Trump] We really do.

138
00:08:05.360 --> 00:08:06.960
[Trump] This is just absolutely nuts.

139
00:08:07.480 --> 00:08:11.840
[Obama] I'm willing to entertain your critiques, Donald, but you should be more courteous.

140
00:08:12.640 --> 00:08:16.520
[Obama] There are people, smart people, trying to help make a better future for us here.

141
00:08:16.780 --> 00:08:18.940
[Trump] Oh, I'm one of the most grateful people.

142
00:08:18.940 --> 00:08:20.420
[Trump] You wouldn't even believe it.

143
00:08:20.560 --> 00:08:25.080
[Trump] I mean, these researchers could be lazing around and playing video games all day long.

144
00:08:25.160 --> 00:08:25.640
[Trump] I'm grateful.

145
00:08:25.840 --> 00:08:26.400
[Trump] I really am.

146
00:08:26.740 --> 00:08:28.600
[Trump] I'm grateful to find people pitching in.

147
00:08:28.860 --> 00:08:32.820
[Trump] People ask me all the time these days, how do you feel about alignment researchers?

148
00:08:33.240 --> 00:08:34.780
[Trump] And I tell them, they're heroes.

149
00:08:34.900 --> 00:08:35.740
[Trump] They really are.

150
00:08:36.100 --> 00:08:38.659
[Trump] At worst, they're doing something about the problem.

151
00:08:39.299 --> 00:08:42.460
[Trump] At best, they're saving us from total destruction like you wouldn't believe.

152
00:08:42.940 --> 00:08:45.180
[Biden] It's good of you to say that, Donald, and I agree.

153
00:08:45.180 --> 00:08:50.980
[Biden] I thank each and every person working honestly, diligently to make our future better and safer.

154
00:08:51.900 --> 00:08:56.520
[Biden] And that includes alignment researchers, especially given the stakes of the challenge.

155
00:08:56.980 --> 00:08:59.660
[Biden] And I thank them from the bottom of my heart.

156
00:08:59.860 --> 00:09:01.840
[Trump] And—Agent Foundations is worthless.

157
00:09:02.260 --> 00:09:03.340
[Biden] And you're delusional.

158
00:09:03.520 --> 00:09:07.160
[Biden] First, a lot of their work has been secret so they don't boost AI capabilities.

159
00:09:07.180 --> 00:09:09.960
[Trump] And so that they don't boost AI alignment either, apparently.

160
00:09:10.320 --> 00:09:22.020
[Biden] Second, rather than throwing a bunch of hacky, a bunch of questionable ideas and schemes at the wall and hoping the problem goes away, Agent Foundations looks for understanding, for meaning.

161
00:09:22.860 --> 00:09:31.440
[Biden] When you find yourself stuck in a deeply confusing problem, the first step is not to flail around and run experiments which use ideas you already understand.

162
00:09:32.120 --> 00:09:35.580
[Biden] It's to look sideways and see if you can't come up with some new ideas.

163
00:09:35.800 --> 00:09:36.680
[Trump] This is so vague.

164
00:09:36.760 --> 00:09:37.960
[Trump] This is unbelievably vague.

165
00:09:37.980 --> 00:09:40.400
[Biden] And Agent Foundations actually tackles these issues.

166
00:09:40.600 --> 00:09:42.000
[Trump] And how have they done at that, Joseph?

167
00:09:42.000 --> 00:09:44.860
[Obama] I'm worried you're going to start talking past each other.

168
00:09:46.240 --> 00:09:48.700
[Obama] I think we should split this out into two propositions.

169
00:09:49.660 --> 00:09:55.940
[Obama] First, Agent Foundations is on the right track to eventually contribute to an alignment solution.

170
00:09:57.060 --> 00:10:06.560
[Obama] Second, and this is what matters for the tier ranking, Agent Foundations is good research to do right now, given the real-world situation we face.

171
00:10:07.140 --> 00:10:07.780
[Biden] Thanks, Barack.

172
00:10:07.920 --> 00:10:09.840
[Biden] I think that this agenda is on the right track.

173
00:10:09.840 --> 00:10:15.160
[Biden] And I think it's a decent, but not perfect, not at all perfect, way to allocate additional research.

174
00:10:15.700 --> 00:10:18.860
[Biden] I think Agent Foundations is on the right track, is on a solid road.

175
00:10:19.320 --> 00:10:25.560
[Biden] It's not enough to just do interpretability and understand what gradient descent is doing, what its inductive biases are.

176
00:10:26.140 --> 00:10:28.560
[Biden] You've got to know what kind of mind you want to get out the other end.

177
00:10:29.080 --> 00:10:33.440
[Biden] You've got to understand how you want your superintelligence to reason, at least at a high level.

178
00:10:33.780 --> 00:10:35.700
[Trump] Oh, yeah, let's just build an omnipotent god.

179
00:10:35.760 --> 00:10:36.600
[Trump] Let's build a singleton.

180
00:10:36.720 --> 00:10:38.160
[Trump] I can't imagine regretting that.

181
00:10:38.160 --> 00:10:39.680
[Biden] Jesus, not this argument again.

182
00:10:39.740 --> 00:10:45.280
[Biden] Anyways, Agent Foundations is an answer to the question, what kind of mind do we want to build anyways?

183
00:10:45.480 --> 00:10:46.880
[Biden] And they've made some progress.

184
00:10:47.060 --> 00:10:49.320
[Biden] Agent Foundations gave us logical induction.

185
00:10:49.860 --> 00:10:53.840
[Trump] I mean, I think they're lost in fantasy land with strange philosophical assumptions.

186
00:10:54.060 --> 00:10:58.060
[Trump] I mean, I tried arguing with one of the researchers, very smart guy, brilliant guy.

187
00:10:58.540 --> 00:11:02.360
[Trump] I tried arguing with him to explain the mistakes, but they just can't see it.

188
00:11:02.740 --> 00:11:03.680
[Obama] Here's what I think.

189
00:11:04.540 --> 00:11:09.760
[Obama] I agree with Donald that publicly available research in Agent Foundations has been practically useless.

190
00:11:10.620 --> 00:11:16.840
[Obama] I think I'm more open to investing in a wide range of alignment worldviews, even if I find them unconvincing.

191
00:11:17.720 --> 00:11:23.500
[Obama] That said, due to how little time I think we have before AGI, I'm leaning towards D-tier at best.

192
00:11:23.900 --> 00:11:24.540
[Trump] Put it in F-tier.

193
00:11:25.000 --> 00:11:26.100
[Trump] You know it belongs, Barack.

194
00:11:26.500 --> 00:11:29.580
[Biden] On my own, I'd give it B or A-tier, but I guess I'll settle for C-tier.

195
00:11:29.700 --> 00:11:30.720
[Trump] I can't believe this guy.

196
00:11:30.780 --> 00:11:31.460
[Trump] I really can't.

197
00:11:31.460 --> 00:11:33.280
[Trump] Can you believe he's supposed to be president?

198
00:11:33.940 --> 00:11:40.760
[Trump] We're staring down the barrel of a giant gun, a huge gun, pointed right at our beautiful nation, and Joe wants to encourage people.

199
00:11:40.880 --> 00:11:41.540
[Trump] He really does.

200
00:11:41.620 --> 00:11:48.160
[Trump] He wants to encourage them to work on an agenda that is yielded like the tiniest piece of progress imaginable, if that.

201
00:11:48.500 --> 00:11:49.000
[Obama] You know what?

202
00:11:49.040 --> 00:11:52.600
[Obama] I still think you're being obnoxious and hyperbolic, Donald, but I agree.

203
00:11:53.300 --> 00:11:55.000
[Obama] This one goes in F-tier.

204
00:11:56.240 --> 00:11:57.280
[Obama] Let me be clear.

205
00:11:58.000 --> 00:12:08.740
[Obama] I applaud the researchers themselves and don't mean to mock or belittle anyone for pursuing an agenda they find promising, but part of being a leader is to speak frankly, even when it's hard.

206
00:12:09.220 --> 00:12:11.080
[Trump] You know, maybe I was wrong about you, Barack.

207
00:12:11.620 --> 00:12:15.980
[Trump] Once I demolish Joe in 2024, I'll need a vice president, and...

208
00:12:15.980 --> 00:12:17.060
[Biden] You're a child, Donald.

209
00:12:17.800 --> 00:12:19.740
[Obama] Let's consider Conjecture's research agenda.

210
00:12:20.220 --> 00:12:20.980
[Biden] What is it?

211
00:12:21.300 --> 00:12:21.840
[Trump] I don't know.

212
00:12:23.260 --> 00:12:24.560
[Obama] So where are we going to put it?

213
00:12:25.000 --> 00:12:25.500
[Trump] S-tier.

214
00:12:25.680 --> 00:12:27.200
[Trump] The S is for socio-hazard.

215
00:12:27.460 --> 00:12:28.060
[Obama + Biden] That's stupid.

216
00:12:29.060 --> 00:12:33.780
[Trump] Wasn't the Conjecture agenda the one trying to, you know, derive some alternative to deep learning?

217
00:12:34.400 --> 00:12:35.040
[Obama] I think so.

218
00:12:35.400 --> 00:12:36.040
[Trump] That's stupid.

219
00:12:36.160 --> 00:12:37.680
[Trump] That's so stupid you wouldn't believe it.

220
00:12:38.420 --> 00:12:50.780
[Trump] These so-called LessWrong folk love talking about the planning fallacy, and then they're like, oh, yeah, let's just derive a new form of safe, transparent optimization, which is AGI competitive within four years.

221
00:12:51.280 --> 00:12:51.760
[Trump] Delusional.

222
00:12:52.380 --> 00:12:54.880
[Trump] Man up and figure out how to make models do what you want.

223
00:12:54.880 --> 00:13:02.580
[Trump] Instead of pretending to believe in some fairy dust plan, that requires skills we won't have in time, which we really won't have in time.

224
00:13:02.840 --> 00:13:07.120
[Obama] As an aside, I think Conjecture considers itself mostly distinct from LessWrong culture.

225
00:13:07.600 --> 00:13:13.040
[Biden] Another red flag, if you're on a date and they don't have a LessWrong account, you know, you know they're trouble.

226
00:13:13.580 --> 00:13:16.440
[Obama] Joe, when would any of us possibly benefit from that advice?

227
00:13:16.900 --> 00:13:18.500
[Biden] Uh, what's the next agenda?

228
00:13:18.800 --> 00:13:20.600
[Obama] We still haven't ranked Conjecture?

229
00:13:20.960 --> 00:13:21.560
[Trump] I don't know.

230
00:13:22.180 --> 00:13:22.560
[Trump] D-tier?

231
00:13:23.220 --> 00:13:25.020
[Trump] This agenda sounds too vague to me.

232
00:13:25.140 --> 00:13:30.100
[Trump] I mean, I know what it takes to make a deal go through, to make a plan go through, and their plan sounds insane to me.

233
00:13:30.240 --> 00:13:34.340
[Trump] Believe me, I'm great at evaluating plans, and this plan doesn't make sense.

234
00:13:35.360 --> 00:13:36.640
[Trump] At least what I've heard of it.

235
00:13:37.180 --> 00:13:42.660
[Trump] Maybe it'd make more sense if Conjecture didn't consider the smallest possible English sentences to be info hazards.

236
00:13:42.980 --> 00:13:44.160
[Trump] I mean, what a joke, guys.

237
00:13:44.240 --> 00:13:50.840
[Trump] I mean, look, I respect them for trying to stop something bad from happening, but this "infosecurity" thing is going too far.

238
00:13:50.840 --> 00:13:51.600
[Trump] I have news.

239
00:13:51.720 --> 00:13:53.980
[Trump] There are lots of capabilities advances every day.

240
00:13:54.120 --> 00:13:57.040
[Trump] There are thousands, absolutely thousands of people.

241
00:13:57.340 --> 00:14:02.260
[Trump] You wouldn't believe how many people are trying to build a giant bomb that will kill them and their own family.

242
00:14:02.640 --> 00:14:04.960
[Trump] These capabilities researchers are maniacs.

243
00:14:05.360 --> 00:14:06.520
[Trump] The pedal is to the metal.

244
00:14:07.180 --> 00:14:19.960
[Trump] And all these alignment researchers are so afraid of people getting mad at them, of people getting mad at them for so-called capabilities advances, they're becoming paralyzed, absolutely paralyzed by fear, by indecision.

245
00:14:19.960 --> 00:14:21.260
[Trump] You hate to see it.

246
00:14:21.540 --> 00:14:23.400
[Trump] Just share your goddamn work.

247
00:14:23.640 --> 00:14:24.900
[Trump] We have real work to do.

248
00:14:24.960 --> 00:14:28.780
[Trump] We have some jobs to create, some alignment jobs, and I want to create them.

249
00:14:28.980 --> 00:14:38.500
[Trump] And that can't happen if everyone's scared to death of sharing the smallest pieces of information about their work, which, like, barely anyone is paying attention to anyways, to be totally honest.

250
00:14:39.080 --> 00:14:40.020
[Obama] Yeah, D-tier it is.

251
00:14:40.800 --> 00:14:44.720
[Obama] Have either of you managed to understand Vanessa Kosoy's infrabayesianism proposal?

252
00:14:45.180 --> 00:14:46.040
[Trump] Obviously not.

253
00:14:46.320 --> 00:14:46.600
[Biden] No.

254
00:14:46.600 --> 00:14:48.960
[Trump] You know, I don't think anyone understands that.

255
00:14:49.080 --> 00:14:50.040
[Trump] I really don't.

256
00:14:50.180 --> 00:14:56.160
[Trump] At Mar-a-Lago, some of my wealthiest friends, smartest friends, you wouldn't believe how sharp these people are.

257
00:14:56.300 --> 00:14:57.380
[Trump] They would ask me for help.

258
00:14:57.660 --> 00:15:04.680
[Trump] And out of everyone, besides the main researchers maybe, I know the most about this agenda, and I just don't understand it.

259
00:15:04.740 --> 00:15:05.840
[Trump] I didn't know what to tell them.

260
00:15:06.440 --> 00:15:07.080
[Trump] I really didn't.

261
00:15:07.220 --> 00:15:11.660
[Trump] Vanessa, smart person, wonderful person, no idea what this is.

262
00:15:11.920 --> 00:15:12.980
[Trump] Let's make a new tier.

263
00:15:14.040 --> 00:15:15.400
[Trump] The WTF tier.

264
00:15:15.940 --> 00:15:16.340
[Obama] Agreed.

265
00:15:16.400 --> 00:15:18.020
[Obama] This is going in WTF tier.

266
00:15:18.400 --> 00:15:20.880
[Trump] I want to talk about that stupid shard theory bullshit.

267
00:15:21.240 --> 00:15:23.000
[Obama] I can't wait to be graced by your perspective.

268
00:15:23.100 --> 00:15:24.840
[Trump] I mean, at first I was really glad to see it.

269
00:15:24.880 --> 00:15:25.520
[Trump] I really was.

270
00:15:25.700 --> 00:15:28.400
[Trump] They were calling out some of the lowest quality reasoning.

271
00:15:28.540 --> 00:15:30.340
[Trump] They finally noticed how bad it was.

272
00:15:30.640 --> 00:15:33.840
[Trump] They were absolutely demolishing that outer alignment stuff.

273
00:15:34.360 --> 00:15:37.180
[Trump] I was happy, maybe for the first time in years.

274
00:15:37.720 --> 00:15:40.200
[Trump] But then they started making a bunch of their own shit up.

275
00:15:40.240 --> 00:15:41.020
[Trump] I couldn't believe it.

276
00:15:41.380 --> 00:15:42.440
[Trump] I thought it was a joke.

277
00:15:42.780 --> 00:15:43.680
[Trump] And I was right.

278
00:15:43.820 --> 00:15:45.000
[Trump] Shard theory is a joke.

279
00:15:45.000 --> 00:15:47.180
[Biden] Donald, I got to admit it.

280
00:15:47.240 --> 00:15:49.880
[Biden] The shard theorists at least force their theory to make predictions.

281
00:15:50.580 --> 00:15:53.880
[Biden] They seem to live in reality, more so than other theorists, I'll admit.

282
00:15:53.960 --> 00:15:54.920
[Trump] That's a pretty low bar.

283
00:15:55.160 --> 00:15:56.480
[Trump] This research field is cringe.

284
00:15:56.680 --> 00:15:58.480
[Obama] Can we talk about the substance of the theory?

285
00:15:59.220 --> 00:16:01.140
[Obama] I think there are two primary claims.

286
00:16:02.020 --> 00:16:13.020
[Obama] First, that AI policies are better understood as modular shards of influence and desire, desires which vary by context, rather than a really smart agent with a fixed single goal.

287
00:16:13.020 --> 00:16:18.800
[Obama] Second, that we should anchor to humans as the one existing example of general intelligence we've ever seen.

288
00:16:18.880 --> 00:16:20.140
[Trump] Oh, AI will be like humans.

289
00:16:20.220 --> 00:16:20.440
[Trump] Great.

290
00:16:20.540 --> 00:16:22.040
[Trump] I'm sure that'll be extremely accurate.

291
00:16:22.500 --> 00:16:26.160
[Biden] I think lots of folks really already agree that fixed single goals are unrealistic.

292
00:16:26.420 --> 00:16:29.540
[Biden] This aspect feels less new, although I see the appeal.

293
00:16:29.800 --> 00:16:31.260
[Trump] Let the hindsight bias begin.

294
00:16:31.520 --> 00:16:33.980
[Trump] I mean, you can't even tell what you're thinking right now, Joe.

295
00:16:34.260 --> 00:16:36.900
[Trump] So I don't know why you'd remember what you used to think.

296
00:16:36.960 --> 00:16:41.180
[Obama] I think the maze interpretability work provided good evidence for the shard frame.

297
00:16:41.180 --> 00:16:46.520
[Obama] You know, that maze-solving network sure seemed to have multiple contextual goals.

298
00:16:47.340 --> 00:16:49.640
[Obama] The AI goes to the cheese when near the cheese.

299
00:16:49.740 --> 00:16:52.180
[Obama] It goes to the corner when the cheese is hard to get to.

300
00:16:52.460 --> 00:17:00.700
[Biden] I liked how they reached into that network and puppeteered it, dragged the AI wherever they wanted in the maze, like a dog on a leash.

301
00:17:01.180 --> 00:17:02.040
[Trump] What the hell, Joe?

302
00:17:02.540 --> 00:17:12.020
[Trump] Anyways, I'm worried that they're fiddling around with tiny networks which won't teach us anything important about these giant, these large, very large, unbelievably large language models.

303
00:17:12.020 --> 00:17:14.480
[Obama] Didn't you read their recent work on controlling GPT-2?

304
00:17:14.920 --> 00:17:15.400
[Trump] Cool trick.

305
00:17:15.480 --> 00:17:16.119
[Trump] Don't get me wrong.

306
00:17:16.240 --> 00:17:16.940
[Trump] Maybe promising.

307
00:17:17.099 --> 00:17:19.079
[Trump] Some folks I know are very excited about that.

308
00:17:19.140 --> 00:17:21.740
[Trump] But what the hell does GPT-2 have to do with shard theory?

309
00:17:21.760 --> 00:17:25.040
[Obama] Apparently the technique was discovered very quickly using shard theory reasoning.

310
00:17:25.540 --> 00:17:25.819
[Trump] Fine.

311
00:17:25.920 --> 00:17:28.520
[Trump] I mean, I got to give the so-called shard theorists props.

312
00:17:28.700 --> 00:17:31.120
[Trump] I was getting real fed up with them, but I'm proud now.

313
00:17:31.200 --> 00:17:32.460
[Trump] I mean, the shard theory itself.

314
00:17:32.980 --> 00:17:34.720
[Trump] Oh, my God, don't get me started on the theory.

315
00:17:35.240 --> 00:17:37.100
[Trump] More armchair philosophical bullshit.

316
00:17:37.100 --> 00:17:40.880
[Trump] Jesus, is it so hard to just pick up a damn model and see what it's actually doing?

317
00:17:41.240 --> 00:17:42.280
[Trump] But then they did it.

318
00:17:42.580 --> 00:17:48.280
[Trump] Maybe for the first time ever in this godforsaken research field, someone made predictions and then ran an experiment.

319
00:17:48.780 --> 00:17:49.960
[Trump] The shard theorists did it.

320
00:17:50.040 --> 00:17:50.680
[Trump] They really did.

321
00:17:50.760 --> 00:17:51.140
[Trump] Incredible.

322
00:17:51.620 --> 00:17:51.880
[Trump] Wow.

323
00:17:52.460 --> 00:17:55.260
[Obama] The sad thing about you is that I can't even tell if you're joking.

324
00:17:55.740 --> 00:18:02.100
[Biden] I think they're ignoring the hardest issues like reflection and just focusing on the easy experiments which we can measure today.

325
00:18:02.120 --> 00:18:04.260
[Trump] I think that's called living in reality, Joseph.

326
00:18:04.860 --> 00:18:18.420
[Trump] Instead of making up embarrassing arguments, some of the worst and most tenuous arguments, I mean, and I'm just trying to be frank, how many times do you have to shoot yourself in the face before you learn to run experiments?

327
00:18:19.040 --> 00:18:19.440
[Biden] I agree.

328
00:18:19.680 --> 00:18:20.520
[Biden] I mean, Anthropic.

329
00:18:20.620 --> 00:18:22.840
[Biden] They're racing ahead so quickly, it's very irresponsible.

330
00:18:23.320 --> 00:18:23.760
[Trump] LMAO.

331
00:18:23.840 --> 00:18:24.820
[Trump] There you have it, folks.

332
00:18:24.960 --> 00:18:27.460
[Trump] And this is Sleepy Joe at his best, his absolute best.

333
00:18:27.820 --> 00:18:28.560
[Obama] Are you OK, Joe?

334
00:18:28.620 --> 00:18:29.920
[Obama] We aren't talking about Anthropic.

335
00:18:29.980 --> 00:18:31.280
[Obama] We're discussing shard theory.

336
00:18:31.900 --> 00:18:32.340
[Biden] Right.

337
00:18:32.540 --> 00:18:32.720
[Biden] Thanks.

338
00:18:32.820 --> 00:18:33.540
[Biden] I find it appealing.

339
00:18:33.700 --> 00:18:34.400
[Biden] The theory is nice.

340
00:18:35.000 --> 00:18:37.040
[Trump] Theory is useless, but they got some results.

341
00:18:37.640 --> 00:18:44.920
[Trump] I was talking with some folks, some of the richest businessmen and smartest intellectuals, and that GPT-2 work was just the talk of the town.

342
00:18:45.520 --> 00:18:50.560
[Trump] God, I couldn't even order my food because the server asked me about that blog post and he wouldn't stop.

343
00:18:50.840 --> 00:18:52.960
[Obama] Donald, stop telling me about the people you're talking to.

344
00:18:53.700 --> 00:18:55.760
[Obama] How do you even have time to talk to that many people?

345
00:18:56.060 --> 00:18:57.680
[Biden] I think this should go in A tier.

346
00:18:57.680 --> 00:18:58.480
[Trump] Nice try.

347
00:18:58.580 --> 00:18:59.680
[Trump] This is B tier at best.

348
00:18:59.800 --> 00:19:01.860
[Trump] Let's see how their experimental results pan out.

349
00:19:02.500 --> 00:19:04.280
[Obama] I think this goes in low A tier.

350
00:19:05.260 --> 00:19:10.660
[Obama] There's more variance here than usual, mostly because it's unclear how many of their claims and intuitions will pan out.

351
00:19:11.220 --> 00:19:16.300
[Obama] I find shard theory perspectives refreshing, really a different frame on the whole problem.

352
00:19:16.860 --> 00:19:23.980
[Obama] But I'm concerned by lack of formality in the theoretical claims, and it's not clear how important the GPT-2 work will end up being.

353
00:19:23.980 --> 00:19:27.820
[Biden] Let's talk about the eliciting latent knowledge agenda.

354
00:19:28.880 --> 00:19:36.480
[Biden] After years of flailing around at specifying complex objectives, Paul Christiano decided to pose a simple problem.

355
00:19:37.440 --> 00:19:43.320
[Biden] Get a smart AI to honestly report whether it thinks a diamond is still present in a room.

356
00:19:43.720 --> 00:19:44.120
[Trump] Yawn.

357
00:19:44.360 --> 00:19:57.280
[Biden] Donald, this problem might be hard because the AI might be thinking in alien ways, which are hard to supervise or understand, and maybe it's easier for the model to learn to just say whether a person would think the diamond's still there.

358
00:19:57.720 --> 00:19:58.760
[Trump] Maybe it's easier, huh?

359
00:19:59.040 --> 00:19:59.900
[Trump] Maybe, maybe, maybe.

360
00:20:00.080 --> 00:20:07.740
[Trump] We're spending all this time and attention on more armchair theory, but look, guys, how do we know that this is even an issue?

361
00:20:08.200 --> 00:20:10.500
[Obama] Seems good, you know, if we can get models to be truthful.

362
00:20:10.920 --> 00:20:12.020
[Trump] I'm not critiquing that.

363
00:20:12.120 --> 00:20:14.700
[Trump] I mean, that Collin Burns stuff was really good.

364
00:20:14.760 --> 00:20:16.900
[Trump] I'm talking about these so-called theorists.

365
00:20:17.020 --> 00:20:19.480
[Biden] You have anything more substantial to critique here, Donald?

366
00:20:19.480 --> 00:20:25.100
[Trump] Getting them truthful, fine, but how are these researchers, you know, how are they actually approaching the problem?

367
00:20:25.600 --> 00:20:26.520
[Trump] Worst-case analysis.

368
00:20:27.020 --> 00:20:28.200
[Trump] More like worst methodology.

369
00:20:29.020 --> 00:20:31.180
[Trump] What's the assumption again?

370
00:20:31.300 --> 00:20:34.740
[Trump] More of that "minimal loss algorithms get picked" bullshit?

371
00:20:35.340 --> 00:20:36.980
[Trump] This is bigly not true.

372
00:20:37.240 --> 00:20:40.360
[Trump] I mean, we aren't even training these models to convergence.

373
00:20:40.640 --> 00:20:42.160
[Trump] Just read a paper on scaling.

374
00:20:42.700 --> 00:20:45.780
[Trump] You can look at knowledge distillation as well.

375
00:20:46.320 --> 00:20:48.120
[Trump] I mean, folks come up to me all the time.

376
00:20:48.120 --> 00:20:49.320
[Trump] Everyone knows this.

377
00:20:49.800 --> 00:20:52.200
[Trump] Everyone's talking about how wrong that shit is.

378
00:20:52.640 --> 00:20:53.740
[Trump] Great assumption.

379
00:20:54.500 --> 00:20:56.280
[Trump] And look, we've got this enormous document.

380
00:20:56.420 --> 00:20:57.960
[Trump] You wouldn't believe how long it is.

381
00:20:58.380 --> 00:21:02.740
[Trump] Going into this absolutely insane theoretical depth and detail.

382
00:21:03.160 --> 00:21:05.980
[Trump] Want to guess how many experiments there are?

383
00:21:06.440 --> 00:21:08.000
[Biden] They're doing worst-case analysis, Donald.

384
00:21:09.540 --> 00:21:13.420
[Obama] I think they were also, you know, stating a problem for the community to work on.

385
00:21:13.960 --> 00:21:14.560
[Trump] You're so right.

386
00:21:14.620 --> 00:21:17.040
[Trump] This must be the most concise LessWrong content.

387
00:21:17.040 --> 00:21:19.800
[Trump] Stating a problem in a thousand goddamn pages.

388
00:21:20.200 --> 00:21:21.720
[Obama] Let me be clear.

389
00:21:21.820 --> 00:21:23.120
[Obama] This problem seems quite real.

390
00:21:24.740 --> 00:21:26.900
[Obama] Larger models tend to be more sycophantic.

391
00:21:27.220 --> 00:21:31.220
[Trump] Barack, I'm an expert, an absolute expert on sycophants.

392
00:21:31.360 --> 00:21:31.840
[Trump] Believe me.

393
00:21:32.060 --> 00:21:32.640
[Trump] It's real.

394
00:21:32.740 --> 00:21:33.380
[Trump] It's very real.

395
00:21:33.600 --> 00:21:35.200
[Trump] But I just don't buy that.

396
00:21:35.380 --> 00:21:37.060
[Trump] You know, we know what the fuck we're doing.

397
00:21:37.340 --> 00:21:40.680
[Trump] That we're actually good enough to theorize like this.

398
00:21:40.760 --> 00:21:49.640
[Trump] My God, guys, we can't be wasting time going off on long chains of abstract reasoning, ruling out all kinds of solutions.

399
00:21:49.940 --> 00:21:55.760
[Trump] I mean, they're really ruling out methods left and right in that document without much grounding at all.

400
00:21:55.880 --> 00:21:57.240
[Biden] I think you're missing the point.

401
00:21:57.940 --> 00:22:00.900
[Biden] We can't run experiments on the really important stuff now.

402
00:22:01.260 --> 00:22:03.080
[Trump] This is totally D-tier content.

403
00:22:03.440 --> 00:22:07.100
[Obama] But the only other thing we have in D-tier is Conjecture's agenda.

404
00:22:08.420 --> 00:22:10.380
[Obama] Do you really think those are comparable?

405
00:22:10.760 --> 00:22:12.400
[Trump] Well, I asked Claude to check for me.

406
00:22:12.400 --> 00:22:15.620
[Trump] These guys didn't run any experiments, but Conjecture runs experiments.

407
00:22:16.740 --> 00:22:18.100
[Trump] So you're right, Barack.

408
00:22:18.260 --> 00:22:20.520
[Trump] This belongs in high F-tier.

409
00:22:20.840 --> 00:22:21.680
[Biden] No fucking way.

410
00:22:21.940 --> 00:22:22.900
[Obama] Yeah, for real, Donald.

411
00:22:23.040 --> 00:22:24.580
[Obama] What the fuck are you talking about?

412
00:22:24.960 --> 00:22:31.380
[Trump] Once you look past the prestige associated with this agenda, you might notice how pointless it is.

413
00:22:31.640 --> 00:22:34.240
[Obama] Why do I get the feeling you aren't going to shut up about this?

414
00:22:34.520 --> 00:22:35.380
[Trump] Because I'm not.

415
00:22:35.700 --> 00:22:36.520
[Obama] I've got the mouse.

416
00:22:36.840 --> 00:22:38.020
[Trump] I'm coming to get it.

417
00:22:39.660 --> 00:22:40.680
[Obama] What the hell, man?

418
00:22:40.720 --> 00:22:41.640
[Obama] Stay in your own room.

419
00:22:41.640 --> 00:22:43.120
[Trump] Out of my goddamn way.

420
00:22:43.440 --> 00:22:45.820
[Obama] I'm telling you, man, people are not going to like this.

421
00:22:46.220 --> 00:22:47.520
[Biden] But I think it should go in—

422
00:22:47.780 --> 00:22:48.900
[Trump] I think we're done here.
