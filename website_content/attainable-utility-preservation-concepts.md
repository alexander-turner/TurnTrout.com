---
permalink: attainable-utility-preservation-concepts
lw-was-draft-post: "false"
lw-is-af: "true"
lw-is-debate: "false"
lw-page-url: 
  https://www.lesswrong.com/posts/75oMAADr4265AGK3L/attainable-utility-preservation-concepts
lw-is-question: "false"
lw-posted-at: 2020-02-17T05:20:09.567000Z
lw-last-modification: 2021-05-24T18:19:29.122000Z
lw-curation-date: None
lw-frontpage-date: 2020-02-17T06:30:53.270000Z
lw-was-unlisted: "false"
lw-is-shortform: "false"
lw-num-comments-on-upload: 20
lw-base-score: 38
lw-vote-count: 11
af-base-score: 17
af-num-comments-on-upload: 11
publish: true
title: "Attainable Utility Preservation: Concepts"
lw-latest-edit: 2020-02-17T05:22:28.687000Z
lw-is-linkpost: "false"
tags:
  - "impact-regularization"
  - "AI"
aliases:
  - "attainable-utility-preservation-concepts"
lw-sequence-title: "Reframing Impact"
lw-sequence-image-grid: sequencesgrid/izfzehxanx48hvf10lnl
lw-sequence-image-banner: sequences/zpia9omq0zfhpeyshvev
sequence-link: posts#reframing-impact
prev-post-slug: the-catastrophic-convergence-conjecture
prev-post-title: "The Catastrophic Convergence Conjecture"
next-post-slug: attainable-utility-preservation-empirical-results
next-post-title: "Attainable Utility Preservation: Empirical Results"
lw-reward-post-warning: "false"
use-full-width-images: "false"
date_published: 2020-02-17 00:00:00
original_url: 
  https://www.lesswrong.com/posts/75oMAADr4265AGK3L/attainable-utility-preservation-concepts
skip_import: true
card_image: https://assets.turntrout.com/static/images/card_images/KPv2beS.png
description: "Exploring the ideas behind Attainable Utility Preservation: penalize
  the AI for gaining power to bound its impact."
date_updated: 2025-06-03 22:57:00.423836
---






![Handwritten text reads: "Last time, on Reframing Impact:". Below, a cloud-shaped bubble defines the "Catastrophic Convergence Conjecture": "Unaligned goals tend to have catastrophe-inducing optimal policies because of power-seeking incentives."](https://assets.turntrout.com/static/images/posts/hTnYTsJ.avif)

![Text: "If the CCC is right, then _if_ power gain is disincentivized, the agent isn't incentivized to overfit and disrupt our AU landscape. Without even knowing who we are or what we want, the agent's actions preserve our attainable utilities." Below are illustrated examples of commands: "Make paperclips," "Put the strawberry on the plate," or "Paint the car pink," followed by the main constraint: "... but don't gain power."](https://assets.turntrout.com/static/images/posts/gwVocUy.avif) ![Title text reads: "This approach is called Attainable Utility Preservation." A diagram shows a robot's reach being stopped by a blue energy field. Beside it, three status bars labeled "Human", "Trout", and "AI" all show high attainable utility levels.](https://assets.turntrout.com/static/images/posts/KPv2beS.avif)

![Handwritten text: "imagine an agent receiving reward for a primary task minus a scaled penalty for how much its actions change its power." It specifies this is "AUP_conceptual, not" (underlined) "any formalization you may be familiar with."](https://assets.turntrout.com/static/images/posts/MYNBKOe.avif)

![A diagram asking what a paperclip-manufacturing AUP_conceptual agent might do. Disallowed actions, crossed out with red Xs, are "Build lots of factories," "Copy itself," and "Nothing." The encouraged policy is to "Narrowly improve paperclip production efficiency." Text explains AUP_conceptual is designed to encourage this, and the optimal policy won't be catastrophic.](https://assets.turntrout.com/static/images/posts/ZK2qYPZ.avif)

![Handwritten text: "AUP_conceptual dissolves thorny problems in impact measurement." The question "Is the agent's ontology reasonable?" is answered with "Who cares."](https://assets.turntrout.com/static/images/posts/lk8Keid.avif) ![A robot in a thinking pose sits on a large green circle. Text explains: "Instead of regulating its complex physical effects on the outside world, the agent is looking inwards at itself and its own abilities." A thought bubble shows the robot picturing itself and a power meter.](https://assets.turntrout.com/static/images/posts/kMBZK6d.avif) ![A diagram on the "locality" problem for AI impact penalties. It asks how to avoid penalties from distant state changes, using the example of rearranging inaccessible stars—a huge but irrelevant change. The solution is for AUP_conceptual to regularize the agent's impact on the nearby AU landscape.](https://assets.turntrout.com/static/images/posts/FXlUiYj.avif)

![A diagram asks, "What about butterfly effects? How can the agent possibly determine which effects it's responsible for?" An agent in a maze chooses between going north (somehow causing a tornado in the east) and going south (somehow causing a volcano to erupt in the west). The diagram concludes, "Forget about it."](https://assets.turntrout.com/static/images/posts/hHVvk0Q.avif) ![Handwritten text: "AUP conceptual agents are respectful and conservative with respect to their AU landscape..." A robot interacts with the environment while green bars for "Human," "Trout," and "AI" indicate preserved attainable utilities for these agents.](https://assets.turntrout.com/static/images/posts/3NMSHHl.avif) ![](https://assets.turntrout.com/static/images/posts/BtzHnUq.avif)

![Handwritten text asks, "How can an idea go wrong?" It describes two gaps: one between what we want and the concept, and another between the concept and execution. It then critiques past impact measures for focusing on minimizing physical change or maintaining world states, questioning if this is well-aimed.](https://assets.turntrout.com/static/images/posts/MzW64A5.avif) ![A cartoon robot with a box head is surrounded by red laser-like tripwires. Text above reads: "The hope is that in order for the agent to have a large impact on us, it has to snap a tripwire." Text below reads: "The problem is... well, it's not clear how we could possibly know whether the agent can still find a catastrophic policy; in a sense, the agent is still trying to sneak by the restrictions and gain power over us. An agent maximizing expected utility while actually minimally changing the physical world still probably leads to catastrophe."](https://assets.turntrout.com/static/images/posts/mOWK65o.avif)

![Handwritten text: "That doesn't seem to be the case for AUP conceptual. Assuming CCC, an agent which doesn't gain much power doesn't cause catastrophes. This has no dependency on complicated human value, and most realistic tasks should have reasonable, high-reward policies not gaining undue power."](https://assets.turntrout.com/static/images/posts/VDQiChW.avif) ![A hand-drawn diagram states "So AUP_conceptual meets our desiderata:". Inside a cloud bubble is a list titled "The distance measure should: 1) Be easy to specify, 2) Put catastrophes far away, 3) Put reasonable plans nearby." Next to the list is a "no" symbol over a nervous robot sweating in front of a devilish pink smiley face.](https://assets.turntrout.com/static/images/posts/jtxMXJe.avif)

![Top text: "Therefore, I consider AUP to conceptually be a solution to impact measurement." with a cartoon robot popping champagne. Bottom text: "Wait! let's not get ahead of ourselves! I don't think we've fully bridged the concept / execution gap. However, for AUP, it seems possible – more on that later."](https://assets.turntrout.com/static/images/posts/7KcMK3J.avif)

## Appendix: No free impact

What if we want the agent to single-handedly ensure the future is stable and aligned with our values? AUP probably won’t allow policies which actually accomplish this goal – one needs power to e.g. nip unaligned superintelligences in the bud. AUP aims to prevent catastrophes by stopping bad agents from gaining power to do bad things, but it symmetrically impedes otherwise-good agents.

This doesn’t mean we can’t get useful work out of agents – there are important asymmetries provided by both the main reward function and AU landscape counterfactuals.

First, even though we can’t specify an _aligned_ reward function, the provided reward function still gives the agent useful information about what we want. If we need paperclips, then a paperclip-AUP agent prefers policies which make some paperclips. Simple.

Second, if we don’t like what it’s beginning to do, we can shut it off (because it hasn’t gained power over us). Therefore, it has “approval incentives” which bias it towards AU landscapes in which its power hasn’t decreased too much, either.

So we can hope to build a non-catastrophic AUP agent and get useful work out of it. We just can’t directly ask it to solve all of our problems: it doesn’t make much sense to speak of a “low-impact [singleton](https://lesswrong.com/tag/singleton)”.

# Notes

- To emphasize, when I say "AUP agents do $X$" in this post, I mean that AUP agents correctly implementing the _concept of AUP_ tend to behave in a certain way.
- As [pointed out by Daniel Filan](https://www.lesswrong.com/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure#jJrCTRwTZDZDc3XLx), AUP suggests that one might work better in groups by ensuring one's actions preserve teammates' AUs.
